---
title: "GEO 200CN Lab 2"
author: "Kenneth Larrieu"
date: "April 10, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Question 1.1

The primary difference between the two maps used in OSU vs. rspatial is that the rspatial map used is much higher resolution. As a result, even when the same yardstick is used, the rspatial map results in a longer coastline length.

#### Question 2.1: Create these matrices from `ind` and `dep` without using `byrow=TRUE`.

```{r}
# independent variable
ind <- c(87, 95, 72, 37, 44, 24,
         40, 55, 55, 38, 88, 34,
         41, 30, 26, 35, 38, 24,
         14, 56, 37, 34,  8, 18,
         49, 44, 51, 67, 17, 37,
         55, 25, 33, 32, 59, 54)
# dependent variable
dep <- c(72, 75, 85, 29, 58, 30,
         50, 60, 49, 46, 84, 23,
         21, 46, 22, 42, 45, 14,
         19, 36, 48, 23,  8, 29,
         38, 47, 52, 52, 22, 48,
         58, 40, 46, 38, 35, 55)

# create matrices filling by column (default), then transpose
mi <- t(matrix(ind, ncol=6, nrow=6))
md <- t(matrix(dep, ncol=6, nrow=6))
```

#### Question 2.2: Instead of the `mean` function What other functions could, in principle, reasonably be used in an aggregation of raster cells?

Other functions that could be reasonably used to aggregate raster data include `sum`, `range`, `min`, `max`, `sd`, etc.

#### Question 2.3: There are other ways to do the above (converting two RasterLayer objects to a `data.frame`). Show how to obtain the same result (`d1`) using `as.vector` and `cbind`.

```{r}
# load package
library(raster)
# turn matrices into RasterLayer objects
ri = raster(mi)
rd = raster(md)

# aggregate individually
ai1 = aggregate(ri, c(2, 1), fun=mean)
ad1 = aggregate(rd, c(2, 1), fun=mean)

# then combine into data.frame
d1 = data.frame(ind=as.vector(ai1), dep=as.vector(ad1))
d1
```

#### Question 2.4: Show R code to make a cluster dendogram summarizing the distances between these six sites, and plot it.

```{r}
A = c(40, 43)
B = c(1, 101)
C = c(54, 111)
D = c(104, 65)
E = c(60, 22)
F = c(20, 2)
# create data.frame of point coordinates
pts = rbind(A,B,C,D,E,F)
# create distance matrix
dis = dist(pts)
D = as.matrix(dis)
D = round(D)
# create dendrogram object
hc = hclust(dis)
plot(hc, xlab='site', ylab='distance', sub='')
```

#### Question 2.5: Show how you can do 'column-normalization' (Just an exercise, in spatial data analysis this is not a typical thing to do).

```{r}
# create weights matrix
W = 1 / D
W[!is.finite(W)] = NA
W = round(W, 4)
# get column totals
ctot = colSums(W, na.rm=TRUE)
# divide entries in each column by corresponding column total
W = W / rep(ctot, each=nrow(W))
# check that columns are now normalized
colSums(W, na.rm=TRUE)
```

#### Question 2.6: The `SpatialPolygonDataFrame` class is defined in package sp. But we never used library('sp') . So how is it possible that we have one of such objects?

R automatically installs dependencies that are missing when a package is imported from the library. While `sp` was not imported explicitly, it was imported as a dependency of `dismo`.

#### ISLR 2.4.9:

```{r, echo=FALSE}
library(ISLR)
```

*a.* Which of the predictors are quantitative, and which are qualitative?

```{r}
head(Auto)
```
From looking at the data, we see all values are quantitative except for "origin" and "name". While "origin" is numeric, it represents a physical location and not an actual quantitative measure.

*b.* What is the range of each quantitative predictor?

```{r}
# only first 7 columns (quantitative)
autoq = Auto[1:7]
# apply rannge function column-wise
apply(autoq, 2, range)
```

*c.* What is the mean and standard deviation of each quantitative
predictor?

mean:
```{r}
apply(autoq, 2, mean)
```

standard deviation:
```{r}
apply(autoq, 2, sd)
```

*d.* Now remove the 10th through 85th observations. What is the
range, mean, and standard deviation of each predictor in the
subset of the data that remains?

```{r}
autorm = autoq[, -10:-85]
```

range:

```{r}
apply(autorm, 2, range)
```

mean:

```{r}
apply(autorm, 2, mean)
```

standard deviation:

```{r}
apply(autorm, 2, sd)
```

*e.* Using the full data set, investigate the predictors graphically,
using scatterplots or other tools of your choice. Create some plots
highlighting the relationships among the predictors. Comment
on your findings.

```{r}
# plot all pairs of variables in autorm
pairs(autorm)
```

From the above plot, we see that some pairs of variables appear to have a strong linear relationship (e.g. horsepower and weight) while other pairs appear to be weakly related or unrelated (e.g. weight and year).

*f.* Suppose that we wish to predict gas mileage (mpg) on the basis
of the other variables. Do your plots suggest that any of the
other variables might be useful in predicting mpg? Justify your
answer.

Going off a visual interpretation of the plots, the strongest predictors of mpg appear to be displacement, horsepower, and weight. All three of these variables appear to negatively effect mpg, which intuitively makes sense as well.

#### ISLR 3.7.8:

*a.* Use the `lm()` function to perform a simple linear regression with
`mpg` as the response and `horsepower` as the predictor. Use the
`summary()` function to print the results. Comment on the output.

```{r}
mpg = Auto$mpg
horsepower = Auto$horsepower
m = lm(mpg ~ horsepower)
summary(m)
```

Due to the $p \ll 0.05$, we can reject the null hypothesis and conclude there is a negative relationship between mpg and horsepower. Increasing the horsepower by $1$ approximately decreases the mpg by $0.15$. Thus, due to the range of variability of horsepower, it has a strong influence on mpg, e.g. for a difference of only $10$ horsepower implies a notable difference of $1.5$ miles per gallon. For a horsepower of $98$, the predicted mpg is (along with corresponding confidence and prediction intervals at the $95%$ level):

```{r}
# print predicted value with confidence intervals
predict(m, data.frame(horsepower=c(98)), interval='confidence')
# print predicted value with prediction intervals
predict(m, data.frame(horsepower=c(98)), interval='prediction')
```

*b.* Plot the response and the predictor. Use the `abline()` function
to display the least squares regression line.

```{r}
# data scatter plot
plot(horsepower, mpg)
# plot line of best fit using model coefficients
abline(coef(m))
```

*c.* Use the `plot()` function to produce diagnostic plots of the least
squares regression fit. Comment on any problems you see with
the fit.

```{r}
par(mfrow=c(2,2))
plot(m)
```

Looking at the residual plot, a trend seems to still be present, indicating that the underlying relationship may be nonlinear. Thus, a linear model may not be appropriate. Near the ends of the Q-Q plot we see deviation from the line $y=x$, also indicating the same trend departing from linearity (the trend has a concave behavior). From the residuals and leverage plots, it is evident that most points have a very small amount of leverage, which means the model would be relatively stable if a point were removed. Point 116 has fairly high leverage and residual values compared to the others, while point 331 may be considered an outlier.

#### ISLR 3.7.9: 

*a.* Produce a scatterplot matrix which includes all of the variables
in the data set.

```{r}
pairs(Auto)
```

*b.* Compute the matrix of correlations between the variables using
the function `cor()`. You will need to exclude the `name` variable, which is qualitative.

```{r}
cor(Auto[-9])
```

*c.* Use the `lm()` function to perform a multiple linear regression
with `mpg` as the response and all other variables except `name` as
the predictors. Use the `summary()` function to print the results.
Comment on the output.

```{r}
m = lm(Auto$mpg ~ Auto$cylinders + Auto$displacement + Auto$horsepower + Auto$weight + Auto$acceleration + Auto$year + Auto$origin)
summary(m)
```

From the model output, we see that there is a statistically significant relationship with mpg for the predictors displacement, weight, year, and origin. The coefficient of $~0.75$ for the year predictor indicates that for an increase in the manufacture year by 1 increases the mpg by $~0.75$ on average. This makes sense as newer cars tend to have higher fuel efficiency.

*d.* Use the `plot()` function to produce diagnostic plots of the linear
regression fit. Comment on any problems you see with the fit.
Do the residual plots suggest any unusually large outliers? Does
the leverage plot identify any observations with unusually high
leverage?

```{r}
par(mfrow=c(2,2))
plot(m)
```

There is a weak trend which is present in the residuals. The Q-Q plot indicates good agreement between standardized residuals and theoretical quantiles, except on the rght end where the highest standardized residuals are larger than expected by the model. While point 321 is the greatest outlier, there are no unusually large outliers. Point 14 has much more leverage than any of the other points, which corresponds to the buick estate wagon (sw), a car with notably high horsepower given it's low weight and year of manufacture.

*e.* Use the `*` and `:` symbols to fit linear regression models with
interaction effects. Do any interactions appear to be statistically
significant?

In the following model, we consider interactions between displacement, horsepower, weight, and acceleration.

```{r}
m = lm(Auto$mpg ~ Auto$cylinders + Auto$displacement * Auto$horsepower * Auto$weight * Auto$acceleration + Auto$year + Auto$origin)
summary(m)
```
From the above results, we see that the only significant interaction term is that between displacement and acceleration, suggesting that changes in the displacement may affect the degree to which acceleration contributes to mpg and vice versa.

The following model only considers interactions between displacement, horsepower, weight and acceleration, but not the individual terms.

```{r}
m = lm(Auto$mpg ~ Auto$cylinders + Auto$displacement : Auto$horsepower : Auto$weight : Auto$acceleration + Auto$year + Auto$origin)
summary(m)
```
Not surprisingly, the lone interaction term is more significant in this model, though the residuals have a wider range, indicating a poorer fit.

*f.* Try a few different transformations of the variables, such as
$\log(X)$, $\sqrt{X}$, $X^2$. Comment on your findings.

Fitting a linear model using $\log(\text{horsepower})$:

```{r}
m = lm(Auto$mpg ~ Auto$cylinders + Auto$displacement + log(Auto$horsepower) + Auto$weight + Auto$acceleration + Auto$year + Auto$origin)
summary(m)
```
This transformation was useful in that there is significant dependence on $\log(\text{horsepower})$ while in the initial model there was no dependence on horsepower. There is an increase in $R^2$ from the initial model as well, which is expected since the log function is essentially bringing the points closer together.

Fitting another model making the additional transformation $\sqrt{\text{cylinders}}$:

```{r}
m = lm(Auto$mpg ~ sqrt(Auto$cylinders) + Auto$displacement + log(Auto$horsepower) + Auto$weight + Auto$acceleration + Auto$year + Auto$origin)
summary(m)
```
Now $\sqrt{\text{cylinders}}$ has significant dependence as well, and there is a slight increase in $R^2$.

If we instead use $\text{cylinders}^2$:

```{r}
cy2 = Auto$cylinders^2
m = lm(Auto$mpg ~ cy2 + Auto$displacement + log(Auto$horsepower) + Auto$weight + Auto$acceleration + Auto$year + Auto$origin)
summary(m)
```
In this case the transformation decreased $R^2$, and the dependence on $\text{cylinders}^2$ is not significant.
