---
title: "GEO 200CN Lab 7"
author: "Kenneth Larrieu"
date: "May 13, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## ISLR Ch. 5 Question 5

5. In Chapter 4, we used logistic regression to predict the probability of `default` using `income` and `balance` on the `Default` data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.

(a) Fit a logistic regression model that uses `income` and `balance` to predict `default`.

```{r}
set.seed(12345)
# load input data
library(ISLR)
def = ISLR::Default
# fit a logistic regression model
glm.fit = glm(default~income+balance, data=def, family=binomial)
summary(glm.fit)
```

(b) Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:
  - i. Split the sample set into a training set and a validation set.
  
```{r}
library(dismo)
# partition the data set
k = kfold(def, k=2)
train = def[k == 1,]
test = def[k == 2,]
```

  -ii. Fit a multiple logistic regression model using only the training observations.
  
```{r}
# fit logistic regression on training data
glm.fit = glm(default~income+balance, data=train, family=binomial)
summary(glm.fit)
```
  
  -iii. Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.
  
```{r}
# get posterior probabilities for test data
pred_probs = predict(glm.fit, test, type='response')
pred_probs = as.matrix(pred_probs)
# function determines whether default or not based on probability
yesno = function(p){
  if (p>0.5){
    'Yes'
  }
  else{
    'No'
  }
}
# make column of predicted default on test data set
test$def_pred = apply(pred_probs, 1, yesno)
```
  
  -iv. Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.
  
```{r}
# misclassification rate
sum(test$default != test$def_pred)/nrow(test)
```
  
(c) Repeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained.

```{r}
# make a function to repeat the process for a given sampling seed
err_rate = function(seed){
  set.seed(seed)
  k = kfold(def, k=2)
  train = def[k == 1,]
  test = def[k == 2,]
  glm.fit = glm(default~income+balance, data=train, family=binomial)
  pred_probs = predict(glm.fit, test, type='response')
  pred_probs = as.matrix(pred_probs)
  test$def_pred = apply(pred_probs, 1, yesno)
  
  sum(test$default != test$def_pred)/nrow(test)
}

# get misclassification rates for seeds 1:3
apply(as.matrix(c(1:3)), 1, FUN=err_rate)
```

So based on the observed misclassification rates, it is typically in the range of 2.6-2.8%, suggesting a relatively low variance of our validation error rate for different samplings.

(d) Now consider a logistic regression model that predicts the probability of `default` using `income`, `balance`, and a dummy variable for `student`. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for `student` leads to a reduction in the test error rate.

```{r}
# converts yes/no to binary variable
bin = function(string){
  if (string=='Yes'){
    1
  }
  else{
    0
  }
}

# add studentvar binary variable to our data set
def$studentvar = apply(as.matrix(def$student), 1, FUN=bin)

# same as err_rate function but with studentvar added to model
err_rate2 = function(seed){
  set.seed(seed)
  k = kfold(def, k=2)
  train = def[k == 1,]
  test = def[k == 2,]
  glm.fit = glm(default~income+balance+studentvar, data=train, family=binomial)
  
  pred_probs = predict(glm.fit, test, type='response')
  pred_probs = as.matrix(pred_probs)
  test$def_pred = apply(pred_probs, 1, yesno)
  sum(test$default != test$def_pred)/nrow(test)
}

apply(as.matrix(1:3), 1, err_rate2)
```

Now we see that adding the dummy variable for `student` actually led to a slight increase in our misclassification rates for all seeds 1-3. This may suggest that this model is more biased towards the training data.
