---
title: "GEO 200CN Lab 7"
author: "Kenneth Larrieu"
date: "May 13, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## ISLR Ch. 5 Question 5

5. In Chapter 4, we used logistic regression to predict the probability of `default` using `income` and `balance` on the `Default` data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.

(a) Fit a logistic regression model that uses `income` and `balance` to predict `default`.

```{r}
set.seed(12345)
# load input data
library(ISLR)
def = ISLR::Default
# fit a logistic regression model
glm.fit = glm(default~income+balance, data=def, family=binomial)
summary(glm.fit)
```

(b) Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:
  - i. Split the sample set into a training set and a validation set.
  
```{r}
library(dismo)
# partition the data set
k = kfold(def, k=2)
train = def[k == 1,]
test = def[k == 2,]
```

  -ii. Fit a multiple logistic regression model using only the training observations.
  
```{r}
# fit logistic regression on training data
glm.fit = glm(default~income+balance, data=train, family=binomial)
summary(glm.fit)
```
  
  -iii. Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.
  
```{r}
# get posterior probabilities for test data
pred_probs = predict(glm.fit, test, type='response')
pred_probs = as.matrix(pred_probs)
# function determines whether default or not based on probability
yesno = function(p){
  if (p>0.5){
    'Yes'
  }
  else{
    'No'
  }
}
# make column of predicted default on test data set
test$def_pred = apply(pred_probs, 1, yesno)
```
  
  -iv. Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.
  
```{r}
# misclassification rate
sum(test$default != test$def_pred)/nrow(test)
```
  
(c) Repeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained.

```{r}
# make a function to repeat the process for a given sampling seed
err_rate = function(seed){
  set.seed(seed)
  k = kfold(def, k=2)
  train = def[k == 1,]
  test = def[k == 2,]
  glm.fit = glm(default~income+balance, data=train, family=binomial)
  pred_probs = predict(glm.fit, test, type='response')
  pred_probs = as.matrix(pred_probs)
  test$def_pred = apply(pred_probs, 1, yesno)
  
  sum(test$default != test$def_pred)/nrow(test)
}

# get misclassification rates for seeds 1:3
apply(as.matrix(c(1:3)), 1, FUN=err_rate)
```

So based on the observed misclassification rates, it is typically in the range of 2.6-2.8%, suggesting a relatively low variance of our validation error rate for different samplings.

(d) Now consider a logistic regression model that predicts the probability of `default` using `income`, `balance`, and a dummy variable for `student`. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for `student` leads to a reduction in the test error rate.

```{r}
# converts yes/no to binary variable
bin = function(string){
  if (string=='Yes'){
    1
  }
  else{
    0
  }
}

# add studentvar binary variable to our data set
def$studentvar = apply(as.matrix(def$student), 1, FUN=bin)

# same as err_rate function but with studentvar added to model
err_rate2 = function(seed){
  set.seed(seed)
  k = kfold(def, k=2)
  train = def[k == 1,]
  test = def[k == 2,]
  glm.fit = glm(default~income+balance+studentvar, data=train, family=binomial)
  
  pred_probs = predict(glm.fit, test, type='response')
  pred_probs = as.matrix(pred_probs)
  test$def_pred = apply(pred_probs, 1, yesno)
  sum(test$default != test$def_pred)/nrow(test)
}

apply(as.matrix(1:3), 1, err_rate2)
```

Now we see that adding the dummy variable for `student` actually led to a slight increase in our misclassification rates for all seeds 1-3. This may suggest that this model is more biased towards the training data.

## Part 2: ISLR Chapter 7

```{r}
d = read.csv('temperature.csv')
head(d)
```

```{r}
# get average annual temperature
d$temp = rowMeans(d[,c(6:17)])
plot(sort(d$temp), ylab=expression('Annual Average Temperature ('~degree~'C)'), las=1, xlab='Stations')
```

```{r}
library(raster)
# make points for each location
dsp = SpatialPoints(d[,3:4], proj4string=CRS('+proj=longlat +datum=NAD83'))
# combine with data frame
dsp = SpatialPointsDataFrame(dsp, d)
dsp
```

```{r}
cuts <- c(8,11,14,18,21,25)
pols <- list("sp.polygons", CA, fill = "lightblue")
print(spplot(dsp, 'temp', cuts=cuts, sp.layout=pols,
col.regions=rev(heat.colors(5))))
```

### Null Model

```{r}
# use the mean average annual temp as a null model
tavg = mean(d$temp)
# local deviations from the mean
dsp$diff = dsp$temp - tavg

print(spplot(dsp, 'diff', col.regions=rev(heat.colors(5)),
sp.layout=pols, main = "unexplained variation" ))
```

Computing RMSE for the null model:

```{r}
RMSE = function(observed, predicted){
  sqrt(mean((predicted-observed)^2, na.rm=TRUE))
}

RMSE(d$temp, tavg)
```

```{r}
# transform data to Teale-Albers
TA <- CRS(" +proj=aea +lat_1=34 +lat_2=40.5 +lat_0=0 +lon_0=-120 +x_0=0
+y_0=-4000000 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0")
library(rgdal)
dta <- spTransform(dsp, TA)
cata <- spTransform(CA, TA)

# set sampling for cross-validation
set.seed(5162016)
library(dismo)
k <- kfold(dta)
table(k)

test <- dta[k==1, ]
train <- dta[k!=1, ]
```

## Linear Model

```{r}
# make training dataframe of coordinates and temp
df <- data.frame(coordinates(train), temp=train$temp)
colnames(df)[1:2] = c('x', 'y')

# fit temp as linear function of x and y coords
m <- glm(temp ~ x+y, data=df)
summary(m)
```

**Question 2:** Describe (in statistical terms) and explain (in physical terms) the results shown by `summary(m)`.

The results show that for a linear model the average annual temperature has a non-significant dependence on the x coordinate, while there is a significant dependence on the y coordinate. This indicates physically that the average annual temperature does not vary significantly with longitude, and because the y coefficient is negative, it indicates that at lower y values (i.e. latitudes closer to the equator) the temperatures are typically warmer and vice versa. This agrees with expectations because the Mojave Desert is in the south and typically hotter.

**Question 3:** According to this model. How much does the temperature in California change if you travel 500 miles to the north (show the R code to compute that)

```{r}
coefficients(m)[['y']]*500*1609.34 # degrees/m * mi * m/mi
```

So according to the model, the temperature decreases by approximately 6.98 degrees C when one travels 500 miles north. 

```{r}
# predict on the test data
v <- data.frame(coordinates(test))
colnames(v)[1:2] = c('x', 'y')
p <- predict(m, v)

# compare RMSE for null and linear models
# null model
RMSE(mean(train$temp), test$temp)
# linear model
RMSE(p, test$temp)
```

```{r}
# plot observed vs. predicted test data temps
plot(test$temp, p, xlim=c(5,25), ylim=c(5,25), pch=20,
col='blue', xlab='observed', ylab='predicted')
# for reference: y=x
abline(0,1)
```

```{r}
# 5-fold cross-validation
r <- rep(NA,5)
for (i in 1:5) {
test <- dta[k==i, ]
train <- dta[k!=i, ]
df <- data.frame(coordinates(train), temp=train$temp)
m <- glm(temp ~ ., data=df)
v <- data.frame(coordinates(test))
p <- predict(m, v)
r[i] <- RMSE(p, test$temp)
}
r

mean(r)
```

**Question 4:** Was it important to do 5-fold cross-validation, instead of a single 20-80 split?

Although in this case the first test RMSE was very close to the average, it is always preferred to dp the full 5-fold cross validation over a single 20-80 split because it provides a more reliable estimate of the model performance given different sample subsets of the data. For example, if the second training/test sets alone had been used, it would have estimated test RMSE as 3.22, which could give a false impression that the linear model performs worse than the null model.

Now predicting values for grid cells:

```{r}
# create 10km resolution raster
r <- raster(round(extent(cata)), res=10000, crs=TA)

# get the x coordinates
x <- init(r, v='x')
# set areas outside of CA to NA
x <- mask(x, cata)
# get the y coordinates
y <- init(r, v='y')
# combine the two variables (RasterLayers)
s <- stack(x,y)
names(s) <- c('x', 'y')

df <- data.frame(coordinates(dta), temp=dta$temp)
colnames(df)[1:2] = c('x', 'y')
m <- glm(temp ~ ., data=df)

# can use interpolate instead of predict since it knows to use coordinates
z <- interpolate(r, m)
mask <- mask(z, cata)
zm <- mask(z, mask)
plot(zm)
contour(zm, add=TRUE)
```

```{r}
# alternative model with interaction terms
df <- data.frame(coordinates(dta), temp=dta$temp)
colnames(df)[1:2] = c('x', 'y')
test <- df[k==1, ]
train <- df[k!=1, ]
m <- glm(temp ~ x*y, data=train)
summary(m)

# check AIC and RMSE
AIC(m)

RMSE(predict(m, test), test$temp)

# plot the model predictions
z <- interpolate(r, m)
zm <- mask(z, mask)
plot(zm)
contour(zm, add=TRUE)
```

```{r}
# another alternative model with polynomial terms
m <- glm(temp ~ x + y + I(x^2) + I(y^2), data=df)
summary(m)
AIC(m)
RMSE(predict(m, test), test$temp)

z <- interpolate(r, m)
zm <- mask(z, mask)
plot(zm)
contour(zm, add=TRUE)
```

```{r}
# second-order polynomials and interaction terms
m <- glm(temp ~ poly(x, 2, raw=TRUE) * poly(y, 2, raw=TRUE), data=df)
summary(m)
AIC(m)
RMSE(predict(m, test), test$temp)

z <- interpolate(r, m)
zm <- mask(z, mask)
plot(zm)
contour(zm, add=TRUE)
```

**Question 5:** What is the best model so far? Why?

The model with second order polynomials and interaction terms is the best model so far, because it has the lowest test RMSE as well as the lowest AIC.

**Question 6:** Rerun the last model using (a) ridge regression, and (b) lasso regression. Show the changes in coefficients for three values of lambda; by finishing the code below.

```{r}
f <- temp ~ poly(x, 2, raw=TRUE) * poly(y, 2, raw=TRUE)
x <- model.matrix(f, df)
library(glmnet)
g <- glmnet(x, df$temp)
```